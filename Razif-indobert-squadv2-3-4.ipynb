{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Depedency","metadata":{}},{"cell_type":"code","source":"!pip install wandb datasets transformers torch numpy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer, AutoConfig, AutoTokenizer, DefaultDataCollator\nimport torch\nimport numpy as np\nimport time\nimport shutil\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparamter","metadata":{}},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nDATASET = \"rahmanfadhil/squad_v2_id\"\nMODEL_CHECKPOINT = \"indobenchmark/indobert-base-p1\"\n\nBASE_MODEL = 'IndoBERT'\nMODEL_NAME = \"IndoBERT-SQuADv2\"\n\nHF_TOKEN = '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'\nWANDB_KEY = 'f24435d851b3bd0bc0a590bb865ec8eb173bac59'\n\nPROJECT_NAME = f\"{MODEL_NAME}_{str(time.time()).split('.')[0]}\"\nPROJECT_NAME","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Huggingface Stuff","metadata":{}},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(DATASET)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# correct output shape\ndata_collator = DefaultDataCollator()\ntokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\nconfig = AutoConfig.from_pretrained(MODEL_CHECKPOINT)\nconfig.num_labels = 2\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(MODEL_CHECKPOINT, config=config)\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess SQuAD Indo Dataset","metadata":{"execution":{"iopub.status.busy":"2023-05-30T03:33:41.203239Z","iopub.execute_input":"2023-05-30T03:33:41.203661Z","iopub.status.idle":"2023-05-30T03:33:45.230106Z","shell.execute_reply.started":"2023-05-30T03:33:41.203619Z","shell.execute_reply":"2023-05-30T03:33:45.228936Z"}}},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# only for mockup test\ndataset['train'] = dataset['train'].select(range(100))\ndataset['validation'] = dataset['validation'].select(range(50))\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context = dataset[\"train\"][0][\"context\"]\nquestion = dataset[\"train\"][0][\"question\"]\n\ninputs = tokenizer(question, context)\ntokenizer.decode(inputs[\"input_ids\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=384,\n        truncation=\"only_second\",\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    answers = examples[\"answers\"]\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        answer = answers[i]\n        if len(answer[\"answer_start\"]) == 0:\n            start_positions.append(0)\n            end_positions.append(0)\n            continue \n            \n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label it (0, 0)\n        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train_tokenized = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\ndataset_train_tokenized","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_eval_tokenized = dataset[\"validation\"].map(\n    preprocess_function,\n    batched=True,\n    remove_columns=dataset[\"validation\"].column_names,\n)\ndataset_eval_tokenized","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset, dataset_train_tokenized, dataset_eval_tokenized","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wandb.ai Config","metadata":{}},{"cell_type":"code","source":"wandb.login(key=WANDB_KEY)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# method\nsweep_config = {\n    'method': 'grid'\n}\n\n# full hyperparameters\n# parameters_dict = {\n#     'epochs': {\n#         'values': [10]\n#         },\n#     'batch_size': {\n#         'values': [8, 16]\n#         },\n#     'learning_rate': {\n#         'values': [2e-5, 2e-6]\n#         },\n#     'weight_decay': {\n#         'values': [0.01]\n#     },\n# }\n\n# # hyperparameter experiment 1, 2\n# parameters_dict = {\n#     'epochs': {\n#         'values': [5]\n#         },\n#     'batch_size': {\n#         'values': [8]\n#         },\n#     'learning_rate': {\n#         'values': [2e-5, 2e-6]\n#         },\n#     'weight_decay': {\n#         'values': [0.01]\n#     },\n# }\n\n# hyperparameter experiment 3, 4\nparameters_dict = {\n    'epochs': {\n        'values': [5]\n        },\n    'batch_size': {\n        'values': [16]\n        },\n    'learning_rate': {\n        'values': [2e-5, 2e-6]\n        },\n    'weight_decay': {\n        'values': [0.01]\n    },\n}\n\nsweep_config['parameters'] = parameters_dict\nsweep_config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME)\nsweep_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"working_dir = str(os.getcwd()) + \"/\"\nworking_dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(config=None):\n    with wandb.init(config=config):\n        # set sweep configuration\n        config = wandb.config\n        \n        # model name that will be saved on both wandb and huggingface\n        saved_model_name = f\"{PROJECT_NAME}-{config.batch_size}-{config.learning_rate}-{config.weight_decay}-{config.epochs}\"\n        \n        training_args = TrainingArguments(\n            output_dir=saved_model_name,\n            save_strategy=\"epoch\",\n            evaluation_strategy=\"epoch\",\n            logging_strategy=\"epoch\",\n            learning_rate=config.learning_rate,\n            per_device_train_batch_size=config.batch_size,\n            per_device_eval_batch_size=config.batch_size,\n            num_train_epochs=config.epochs,\n            weight_decay=config.weight_decay,\n            hub_token=HF_TOKEN,\n            report_to=\"wandb\",\n            push_to_hub=True,\n            load_best_model_at_end=True,\n        )\n\n        trainer = Trainer(\n            model=model.to(DEVICE),\n            args=training_args,\n            train_dataset=dataset_train_tokenized,\n            eval_dataset=dataset_eval_tokenized,\n            tokenizer=tokenizer,\n            data_collator=data_collator,\n        )\n\n        trainer.train()\n        trainer.save_model()\n        shutil.rmtree(working_dir + saved_model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.agent(sweep_id, train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}